{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7836307,"sourceType":"datasetVersion","datasetId":4593388}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rasterio\nimport uuid\nimport numpy as np\nimport warnings\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:42:50.149521Z","iopub.execute_input":"2025-02-21T13:42:50.149887Z","iopub.status.idle":"2025-02-21T13:42:53.503288Z","shell.execute_reply.started":"2025-02-21T13:42:50.149863Z","shell.execute_reply":"2025-02-21T13:42:53.502235Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.4.3)\nRequirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (25.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2025.1.31)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"categories = {0: 'Cargo plane', 1: 'Helicopter', 2: 'Small car', 3: 'Bus', 4: 'Truck', 5: 'Motorboat', 6: 'Fishing vessel', 7: 'Dump truck', 8: 'Excavator', 9: 'Building', 10: 'Storage tank', 11: 'Shipping container'}","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:42:56.842516Z","iopub.execute_input":"2025-02-21T13:42:56.842892Z","iopub.status.idle":"2025-02-21T13:42:56.847377Z","shell.execute_reply.started":"2025-02-21T13:42:56.842861Z","shell.execute_reply":"2025-02-21T13:42:56.846397Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport random\nimport rasterio\nimport warnings\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open(filename, 'r')\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img\n    \n# Definir el generador de aumentación de imágenes\ndef image_augmentation(image):\n    datagen = ImageDataGenerator(\n        rotation_range=360,\n        width_shift_range=1, \n        height_shift_range=1,\n        shear_range=1, \n        zoom_range=1, \n        horizontal_flip=True,\n        vertical_flip=True,\n        rescale=1./255,\n        channel_shift_range=1,\n        brightness_range=(0.1, 1.0),\n        fill_mode='nearest'\n    )\n    image = tf.convert_to_tensor(image, dtype=tf.float32)\n    # Aplica una transformación aleatoria a la imagen\n    image = datagen.random_transform(image.numpy())  # Necesitamos convertir a numpy para aplicar ImageDataGenerator\n    return image\n\n\n\n\ndef create_tf_dataset(objs, batch_size, categories, do_shuffle=False):\n    def generator():\n        for filename, obj in objs:\n            # Cargar la imagen y asegurarnos de que tenga el tamaño correcto\n            original_image = load_geoimage(filename)\n            original_image = tf.image.resize(original_image, (224, 224))  # Redimensionar\n\n            # Crear etiqueta en formato one-hot\n            label = np.zeros(len(categories), dtype=np.float32)\n            label[list(categories.values()).index(obj.category)] = 1\n\n            # Convertimos a tensores de TensorFlow\n            original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n\n            # Ocasionalmente usar la imagen aumentada en lugar de la original\n            if random.random() > 0.5:  # 50% probabilidad de usar imagen aumentada\n                original_image = image_augmentation(original_image)\n\n            # Retornar solo UNA imagen por iteración\n            yield original_image, label\n    # Crear dataset de TensorFlow a partir del generador\n    dataset = tf.data.Dataset.from_generator(\n        generator,\n        output_signature=(\n            tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),  # Imagen con tamaño fijo\n            tf.TensorSpec(shape=(len(categories),), dtype=tf.float32)  # Etiqueta one-hot\n        )\n    )\n\n    # Aplicar mezcla, batching y prefetching\n    if do_shuffle:\n        dataset = dataset.shuffle(buffer_size=len(objs))\n    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:03.733660Z","iopub.execute_input":"2025-02-21T13:44:03.733956Z","iopub.status.idle":"2025-02-21T13:44:03.742476Z","shell.execute_reply.started":"2025-02-21T13:44:03.733935Z","shell.execute_reply":"2025-02-21T13:44:03.741644Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show(fig)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:06.660819Z","iopub.execute_input":"2025-02-21T13:44:06.661135Z","iopub.status.idle":"2025-02-21T13:44:06.668313Z","shell.execute_reply.started":"2025-02-21T13:44:06.661108Z","shell.execute_reply":"2025-02-21T13:44:06.667323Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import json\n\n# Load database\njson_file = '/kaggle/input/xview-recognition/xview_recognition/xview_ann_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:08.091105Z","iopub.execute_input":"2025-02-21T13:44:08.091363Z","iopub.status.idle":"2025-02-21T13:44:08.442067Z","shell.execute_reply.started":"2025-02-21T13:44:08.091343Z","shell.execute_reply":"2025-02-21T13:44:08.441378Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n    image = GenericImage('/kaggle/input/xview-recognition/xview_recognition/'+json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    obj = GenericObject()\n    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n    obj.category = json_ann['category_id']\n    # Resampling strategy to reduce training time\n    counts[obj.category] += 1\n    image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:09.637520Z","iopub.execute_input":"2025-02-21T13:44:09.637880Z","iopub.status.idle":"2025-02-21T13:44:09.814914Z","shell.execute_reply.started":"2025-02-21T13:44:09.637855Z","shell.execute_reply":"2025-02-21T13:44:09.814227Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'Cargo plane': 635, 'Helicopter': 70, 'Small car': 4290, 'Bus': 2155, 'Truck': 2746, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 4689, 'Storage tank': 1469, 'Shipping container': 1523}\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"##### 2.- Architecture definition","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\n# Cargar el modelo VGG16 preentrenado en ImageNet, sin las capas superiores\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Congelar las capas del modelo base para que no se entrenen\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Crear el modelo secuencial con la arquitectura deseada\nmodel = Sequential()\nmodel.add(base_model)  # Añadir VGG16 como base\nmodel.add(Flatten())  # Aplanar la salida del modelo base\n\n# Agregar capas densas para fine-tuning\nmodel.add(Dense(128, activation='elu'))\nmodel.add(Dense(64, activation='elu'))\nmodel.add(Dropout(0.2))  # Dropout para evitar sobreajuste\n\n# Capa de salida con la cantidad de categorías\nmodel.add(Dense(len(categories), activation='softmax'))  # Aquí usas el número de clases\n\n\n# Ver el resumen del modelo\nmodel.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:11.542311Z","iopub.execute_input":"2025-02-21T13:44:11.542577Z","iopub.status.idle":"2025-02-21T13:44:11.829276Z","shell.execute_reply.started":"2025-02-21T13:44:11.542556Z","shell.execute_reply":"2025-02-21T13:44:11.828421Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m3,211,392\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │             \u001b[38;5;34m780\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">780</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,935,116\u001b[0m (68.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,935,116</span> (68.42 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,220,428\u001b[0m (12.28 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,220,428</span> (12.28 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n</pre>\n"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD\n\n# Usando SGD con momentum\nopt_sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:14.105471Z","iopub.execute_input":"2025-02-21T13:44:14.105817Z","iopub.status.idle":"2025-02-21T13:44:14.112184Z","shell.execute_reply.started":"2025-02-21T13:44:14.105789Z","shell.execute_reply":"2025-02-21T13:44:14.111435Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\ntrain_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []\n\ndef plot_metrics(epoch, logs):\n    train_loss.append(logs['loss'])\n    val_loss.append(logs['val_loss'])\n    train_acc.append(logs['accuracy'])\n    val_acc.append(logs['val_accuracy'])\n    clear_output(wait=True)\n    plt.figure(figsize=(12, 4))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(train_loss, label='Train Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.title('Loss')\n    plt.legend()\n\n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(train_acc, label='Train Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n    plt.show()\n    \nplot_callback = LambdaCallback(on_epoch_end=plot_metrics)\n\n# Callbacks\nmodel_checkpoint = ModelCheckpoint('/kaggle/working/model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [plot_callback, model_checkpoint, reduce_lr, early_stop, terminate]","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:16.311249Z","iopub.execute_input":"2025-02-21T13:44:16.311523Z","iopub.status.idle":"2025-02-21T13:44:16.319964Z","shell.execute_reply.started":"2025-02-21T13:44:16.311501Z","shell.execute_reply":"2025-02-21T13:44:16.319206Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def split_annotations(anns, train_size=0.7, valid_size=0.15, test_size=0.15):\n    # Mezclar las anotaciones aleatoriamente\n    np.random.shuffle(anns)\n    \n    # Calcular los índices de corte para la división\n    train_idx = int(len(anns) * train_size)\n    valid_idx = int(len(anns) * (train_size + valid_size))\n    \n    # Dividir las anotaciones en entrenamiento, validación y prueba\n    anns_train = anns[:train_idx]\n    anns_valid = anns[train_idx:valid_idx]\n    anns_test = anns[valid_idx:]\n    \n    return anns_train, anns_valid, anns_test\n\n# Dividir las anotaciones en entrenamiento, validación y prueba\nanns_train, anns_valid, anns_test = split_annotations(anns, train_size=0.7, valid_size=0.15, test_size=0.15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:44:18.550901Z","iopub.execute_input":"2025-02-21T13:44:18.551221Z","iopub.status.idle":"2025-02-21T13:44:18.578758Z","shell.execute_reply.started":"2025-02-21T13:44:18.551194Z","shell.execute_reply":"2025-02-21T13:44:18.577674Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Generate the list of objects from annotations\nobjs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\nobjs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n# Generators\nbatch_size = 128 # Change batch size - Assignment 3 (faster)\ntrain_dataset = create_tf_dataset(objs_train, batch_size=batch_size, categories=categories, do_shuffle=True)\nvalid_dataset = create_tf_dataset(objs_valid, batch_size=batch_size, categories=categories, do_shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:20.605030Z","iopub.execute_input":"2025-02-21T13:44:20.605356Z","iopub.status.idle":"2025-02-21T13:44:20.982288Z","shell.execute_reply.started":"2025-02-21T13:44:20.605328Z","shell.execute_reply":"2025-02-21T13:44:20.981668Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import math\nimport numpy as np\nprint('Training model')\nepochs = 20\ntrain_steps = math.ceil(len(objs_train)/batch_size)\nvalid_steps = math.ceil(len(objs_valid)/batch_size)\n\nmodel.compile(optimizer=opt_sgd, loss='categorical_crossentropy', metrics=['accuracy'])\nh = model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=valid_dataset, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n# Best validation model\nbest_idx = int(np.argmax(h.history['val_accuracy']))\nbest_value = np.max(h.history['val_accuracy'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))","metadata":{"execution":{"iopub.status.busy":"2025-02-21T13:44:22.130715Z","iopub.execute_input":"2025-02-21T13:44:22.131031Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training model\nEpoch 1/20\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(h.history.keys())\n\nplt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T12:23:30.543709Z","iopub.status.idle":"2025-02-21T12:23:30.544063Z","shell.execute_reply":"2025-02-21T12:23:30.543916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n#model.load_weights('model.hdf5', by_name=True)\ny_true, y_pred = [], []\nfor ann in anns:\n    # Load image\n    image = load_geoimage(ann.filename)\n    for obj_pred in ann.objects:\n        # Generate prediction\n        warped_image = np.expand_dims(image, 0)\n        predictions = model.predict(warped_image)\n        # Save prediction\n        pred_category = list(categories.values())[np.argmax(predictions)]\n        pred_score = np.max(predictions)\n        y_true.append(obj_pred.category)\n        y_pred.append(pred_category)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T12:23:30.547127Z","iopub.status.idle":"2025-02-21T12:23:30.547468Z","shell.execute_reply":"2025-02-21T12:23:30.547328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\ndraw_confusion_matrix(cm, categories)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T12:23:30.548957Z","iopub.status.idle":"2025-02-21T12:23:30.549349Z","shell.execute_reply":"2025-02-21T12:23:30.549185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Compute the accuracy\ncorrect_samples_class = np.diag(cm).astype(float)\ntotal_samples_class = np.sum(cm, axis=1).astype(float)\ntotal_predicts_class = np.sum(cm, axis=0).astype(float)\nprint('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\nacc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\nprint('Mean Recall: %.3f%%' % (acc.mean() * 100))\nacc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\nprint('Mean Precision: %.3f%%' % (acc.mean() * 100))\nfor idx in range(len(categories)):\n    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n    tp = cm[idx, idx]\n    fp = sum(cm[:, idx]) - tp\n    fn = sum(cm[idx, :]) - tp\n    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n    # Precision: proportion of predicted positive cases that were truly real positives.\n    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))","metadata":{"execution":{"iopub.status.busy":"2025-02-21T12:23:30.550156Z","iopub.status.idle":"2025-02-21T12:23:30.550513Z","shell.execute_reply":"2025-02-21T12:23:30.550390Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Report\n\nYou must prepare a report (PDF) describing:\n* The problems and data sets (briefly).\n* The process that you have followed to reach your solution for the “xview_recognition” benchmark, including your intermediate results. You must discuss and compare these results properly.\n* Final network architectures, including optimization algorithms, regularization methods (dropout, data augmentation, etc.), number of layers/parameters, and performance obtained with your model on the train/valid/test data sets, including the plots of the evolution of losses and accuracy.\n* It would also be very valuable your feedback on the use of “Cesvima” or “Google Colab\" services.\n\nIn the submission via Moodle, attach your Python (.py) or Jupyter Notebook (.ipynb) source file, including in the report all results of computations attached to the code that generated them.\n\nThe assignment must be done in groups of 3 students.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}